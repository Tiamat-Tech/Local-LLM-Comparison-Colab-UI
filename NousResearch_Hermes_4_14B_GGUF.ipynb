{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Troyanovsky/Local-LLM-Comparison-Colab-UI/blob/main/NousResearch_Hermes_4_14B_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkpBtLvuTOp"
      },
      "source": [
        "## NousResearch_Hermes-4-14B-GGUF WebUI\n",
        "\n",
        "1. Run the following cell, takes ~5 min\n",
        "(You may need to confirm to proceed by typing \"Y\")\n",
        "2. Click the gradio link at the bottom\n",
        "3. Chat template: ChatML\n",
        "```\n",
        "<|im_start|>system\n",
        "\n",
        "You are Hermes 4. Be concise and helpful.<|im_end|>\n",
        "<|im_start|>user\n",
        "\n",
        "Explain the photoelectric effect simply.<|im_end|>\n",
        "<|im_start|>assistant\n",
        "```\n",
        "4. Recommended params:\n",
        "```\n",
        "temperature = 0.6\n",
        "top_p = 0.95\n",
        "top_k = 20\n",
        "```\n",
        "\n",
        "Quantized model: https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF\n",
        "\n",
        "Original model: https://huggingface.co/NousResearch/Hermes-4-14B\n",
        "\n",
        "Want to try other local LLMs? Check out this repo: https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-MiHp_bveP6",
        "outputId": "b1f4fad9-931f-42d8-de22-8e6c79737642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'text-generation-webui' already exists and is not an empty directory.\n",
            "/content/text-generation-webui\n",
            "Ignoring triton-windows: markers 'platform_system == \"Windows\"' don't match your environment\n",
            "Ignoring llama-cpp-binaries: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring llama-cpp-binaries: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav3: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav3: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine != \"x86_64\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Requirement already satisfied: accelerate==1.8.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: bitsandbytes==0.46.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 2)) (0.46.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 4)) (4.0.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: fastapi==0.112.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 6)) (0.112.4)\n",
            "Requirement already satisfied: gradio==4.37.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 7)) (4.37.2)\n",
            "Requirement already satisfied: html2text==2025.4.15 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 8)) (2025.4.15)\n",
            "Requirement already satisfied: jinja2==3.1.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 9)) (3.1.6)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 10)) (3.8.2)\n",
            "Requirement already satisfied: numpy==2.2.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 11)) (2.2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 12)) (2.2.2)\n",
            "Requirement already satisfied: peft==0.16.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 13)) (0.16.0)\n",
            "Requirement already satisfied: Pillow>=9.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 14)) (10.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 15)) (5.9.5)\n",
            "Requirement already satisfied: pydantic==2.8.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 16)) (2.8.2)\n",
            "Requirement already satisfied: PyPDF2==3.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 17)) (3.0.1)\n",
            "Requirement already satisfied: python-docx==1.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 18)) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 19)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 20)) (2.32.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 21)) (13.9.4)\n",
            "Requirement already satisfied: safetensors==0.5.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 22)) (0.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 23)) (1.16.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 24)) (0.2.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 25)) (2.19.0)\n",
            "Requirement already satisfied: transformers==4.55.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 26)) (4.55.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 28)) (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 29)) (0.21.3)\n",
            "Requirement already satisfied: flask_cloudflared==0.0.14 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 32)) (0.0.14)\n",
            "Requirement already satisfied: sse-starlette==1.6.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 33)) (1.6.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 34)) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (0.34.4)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.12/dist-packages (from fastapi==0.112.4->-r requirements/full/requirements.txt (line 6)) (0.38.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi==0.112.4->-r requirements/full/requirements.txt (line 6)) (4.15.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.0.2 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (1.0.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (6.5.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (3.10.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (3.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.12.11)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.17.3)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.35.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.8.2->-r requirements/full/requirements.txt (line 16)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.8.2->-r requirements/full/requirements.txt (line 16)) (2.20.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx==1.1.2->-r requirements/full/requirements.txt (line 18)) (5.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.*->-r requirements/full/requirements.txt (line 26)) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.*->-r requirements/full/requirements.txt (line 26)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.*->-r requirements/full/requirements.txt (line 26)) (0.21.4)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.12/dist-packages (from flask_cloudflared==0.0.14->-r requirements/full/requirements.txt (line 32)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.0.2->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (2025.3.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.0.2->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (11.0.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 4)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 4)) (0.70.16)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements/full/requirements.txt (line 12)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements/full/requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements/full/requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements/full/requirements.txt (line 20)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements/full/requirements.txt (line 20)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements/full/requirements.txt (line 20)) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r requirements/full/requirements.txt (line 21)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r requirements/full/requirements.txt (line 21)) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 25)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 25)) (1.74.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 25)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 25)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 25)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 25)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 25)) (3.1.3)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 29)) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 29)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 29)) (4.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 29)) (2.36.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements/full/requirements.txt (line 32)) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements/full/requirements.txt (line 32)) (2.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 4)) (3.12.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements/full/requirements.txt (line 29)) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.1.9)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements/full/requirements.txt (line 21)) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (3.2.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 4)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 4)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 4)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 4)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 4)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 4)) (1.20.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements/full/requirements.txt (line 29)) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 7)) (0.27.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.3.0)\n",
            "Collecting llama-cpp-binaries==0.36.0+cu124\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.36.0/llama_cpp_binaries-0.36.0+cu124-py3-none-linux_x86_64.whl (645.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.3/645.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: llama-cpp-binaries\n",
            "Successfully installed llama-cpp-binaries-0.36.0+cu124\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "9a4e92|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/text-generation-webui/user_data/models/NousResearch_Hermes-4-14B-Q4_K_M.gguf\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "/content/text-generation-webui\n",
            "\u001b[2;36m07:21:39-057860\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting Text generation web UI                        \n",
            "\u001b[2;36m07:21:39-295740\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"NousResearch_Hermes-4-14B-Q4_K_M.gguf\"\u001b[0m        \n",
            "\u001b[2;36m07:21:39-300123\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using \u001b[33mgpu_layers\u001b[0m=\u001b[1;36m999\u001b[0m | \u001b[33mctx_size\u001b[0m=\u001b[1;36m8192\u001b[0m | \u001b[33mcache_type\u001b[0m=\u001b[35mfp16\u001b[0m \n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "build: 1 (e28e303) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "Web UI is disabled\n",
            "main: binding port with default address family\n",
            "main: HTTP server is listening, hostname: 127.0.0.1, port: 56787, http threads: 3\n",
            "main: loading model\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 37 key-value pairs and 443 tensors from user_data/models/NousResearch_Hermes-4-14B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Hermes 4 14B\n",
            "llama_model_loader: - kv   3:                           general.basename str              = Hermes-4\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 14B\n",
            "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = Qwen3 14B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Qwen\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-14B\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,15]      = [\"Qwen-3-14B\", \"instruct\", \"finetune\"...\n",
            "llama_model_loader: - kv  11:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  12:                          qwen3.block_count u32              = 40\n",
            "llama_model_loader: - kv  13:                       qwen3.context_length u32              = 40960\n",
            "llama_model_loader: - kv  14:                     qwen3.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv  15:                  qwen3.feed_forward_length u32              = 17408\n",
            "llama_model_loader: - kv  16:                 qwen3.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv  17:              qwen3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  18:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  19:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  20:                 qwen3.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  21:               qwen3.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {%- set thinking_prompt = 'You are a ...\n",
            "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  32:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  33:                      quantize.imatrix.file str              = /models_out/Hermes-4-14B-GGUF/NousRes...\n",
            "llama_model_loader: - kv  34:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav5.txt\n",
            "llama_model_loader: - kv  35:             quantize.imatrix.entries_count u32              = 280\n",
            "llama_model_loader: - kv  36:              quantize.imatrix.chunks_count u32              = 818\n",
            "llama_model_loader: - type  f32:  161 tensors\n",
            "llama_model_loader: - type q4_K:  241 tensors\n",
            "llama_model_loader: - type q6_K:   41 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 8.38 GiB (4.87 BPW) \n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load:   - 151662 ('<|fim_pad|>')\n",
            "load:   - 151663 ('<|repo_name|>')\n",
            "load:   - 151664 ('<|file_sep|>')\n",
            "load: special tokens cache size = 26\n",
            "load: token to piece cache size = 0.9311 MB\n",
            "print_info: arch             = qwen3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 40960\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 5\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 17408\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 40960\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 14B\n",
            "print_info: model params     = 14.77 B\n",
            "print_info: general.name     = Hermes 4 14B\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 11 ','\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 40 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 41/41 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  8161.75 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   417.30 MiB\n",
            ".\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 8192\n",
            "llama_context: n_ctx_per_seq = 8192\n",
            "llama_context: n_batch       = 256\n",
            "llama_context: n_ubatch      = 256\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (8192) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context:  CUDA_Host  output buffer size =     0.58 MiB\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB\n",
            "llama_kv_cache_unified: size = 1280.00 MiB (  8192 cells,  40 layers,  1/1 seqs), K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
            "llama_context:      CUDA0 compute buffer size =   350.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    15.00 MiB\n",
            "llama_context: graph nodes  = 1566\n",
            "llama_context: graph splits = 2\n",
            "common_init_from_params: added <|endoftext|> logit bias = -inf\n",
            "common_init_from_params: added <|im_end|> logit bias = -inf\n",
            "common_init_from_params: added <|fim_pad|> logit bias = -inf\n",
            "common_init_from_params: added <|repo_name|> logit bias = -inf\n",
            "common_init_from_params: added <|file_sep|> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: model loaded\n",
            "main: chat template, chat_template: {%- set thinking_prompt = 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.' %}\n",
            "{%- set standard_prompt = 'You are Hermes, created by Nous Research.' %}\n",
            "{%- if not thinking is defined %}{% set thinking = false %}{% endif %}\n",
            "{%- if not keep_cots is defined %}{% set keep_cots = false %}{% endif %}\n",
            "{%- if thinking %}{%- set system_prompt = thinking_prompt %}{%- else %}{%- set system_prompt = standard_prompt %}{%- endif %}\n",
            "{%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- system_prompt }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou are a function calling AI model. You may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": \\\"<function-name>\\\", \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\n' + system_prompt + '<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {%- set content = message['content'] -%}\n",
            "            {%- if thinking %}\n",
            "                {%- if not keep_cots %}\n",
            "                    {%- set content = '<think> </think>' + content.split('</think>', 1)[1] -%}\n",
            "                {%- endif %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n' + content + '<|im_end|>' + '\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {%- set content = message['content'] -%}\n",
            "            {%- if thinking %}\n",
            "                {%- if not keep_cots %}\n",
            "                    {%- set content = '<think> </think>' + content.split('</think>', 1)[1] -%}\n",
            "                {%- endif %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n' + content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            ", example_format: '<|im_start|>system\n",
            "You are a helpful assistant<|im_end|>\n",
            "<|im_start|>user\n",
            "Hello<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Hi there<|im_end|>\n",
            "<|im_start|>user\n",
            "How are you?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "'\n",
            "main: server is listening on http://127.0.0.1:56787 - starting the main loop\n",
            "\u001b[2;36m07:22:21-388464\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded \u001b[32m\"NousResearch_Hermes-4-14B-Q4_K_M.gguf\"\u001b[0m in \u001b[1;36m42.09\u001b[0m\n",
            "\u001b[2;36m                \u001b[0m         seconds.                                               \n",
            "\u001b[2;36m07:22:21-389954\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOADER: \u001b[32m\"llama.cpp\"\u001b[0m                                    \n",
            "\u001b[2;36m07:22:21-390937\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRUNCATION LENGTH: \u001b[1;36m8192\u001b[0m                                \n",
            "\u001b[2;36m07:22:21-391747\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INSTRUCTION TEMPLATE: \u001b[32m\"Custom \u001b[0m\u001b[32m(\u001b[0m\u001b[32mobtained from model \u001b[0m    \n",
            "\u001b[2;36m                \u001b[0m         \u001b[32mmetadata\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m                                             \n",
            "\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Running on public URL: https://b5c4ada6bff5d7dfff.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "prompt processing progress, n_past = 40, n_tokens = 40, progress = 1.000000\n",
            "prompt eval time =     639.90 ms /    40 tokens (   16.00 ms per token,    62.51 tokens per second)\n",
            "       eval time =   18137.47 ms /   268 tokens (   67.68 ms per token,    14.78 tokens per second)\n",
            "      total time =   18777.37 ms /   308 tokens\n",
            "\u001b[2;36m07:25:55-772540\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Output generated in \u001b[1;36m18.78\u001b[0m seconds \u001b[1m(\u001b[0m\u001b[1;36m14.21\u001b[0m tokens/s, \u001b[1;36m267\u001b[0m \n",
            "\u001b[2;36m                \u001b[0m         tokens, context \u001b[1;36m40\u001b[0m, seed \u001b[1;36m1861614783\u001b[0m\u001b[1m)\u001b[0m                   \n",
            "\n",
            "prompt eval time =     138.55 ms /    18 tokens (    7.70 ms per token,   129.92 tokens per second)\n",
            "       eval time =   33852.60 ms /   459 tokens (   73.75 ms per token,    13.56 tokens per second)\n",
            "      total time =   33991.15 ms /   477 tokens\n",
            "\u001b[2;36m07:26:46-545098\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Output generated in \u001b[1;36m34.00\u001b[0m seconds \u001b[1m(\u001b[0m\u001b[1;36m13.47\u001b[0m tokens/s, \u001b[1;36m458\u001b[0m \n",
            "\u001b[2;36m                \u001b[0m         tokens, context \u001b[1;36m325\u001b[0m, seed \u001b[1;36m1937406437\u001b[0m\u001b[1m)\u001b[0m                  \n",
            "\u001b[2;36m07:26:52-785671\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Deleted                                                \n",
            "\u001b[2;36m                \u001b[0m         \u001b[32m\"user_data/logs/instruct/20250909-07-24-54.json\"\u001b[0m.      \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 541, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1928, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1514, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 833, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/chat.py\", line 1783, in handle_start_new_chat_click\n",
            "    histories = find_all_histories_with_first_prompts(state)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/chat.py\", line 1209, in find_all_histories_with_first_prompts\n",
            "    with open(path, 'r', encoding='utf-8') as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'user_data/logs/instruct/20250909-07-24-54.json'\n",
            "\n",
            "prompt eval time =     161.98 ms /    35 tokens (    4.63 ms per token,   216.07 tokens per second)\n",
            "       eval time =    1959.05 ms /    28 tokens (   69.97 ms per token,    14.29 tokens per second)\n",
            "      total time =    2121.03 ms /    63 tokens\n",
            "\u001b[2;36m07:49:21-331317\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Output generated in \u001b[1;36m2.13\u001b[0m seconds \u001b[1m(\u001b[0m\u001b[1;36m12.69\u001b[0m tokens/s, \u001b[1;36m27\u001b[0m   \n",
            "\u001b[2;36m                \u001b[0m         tokens, context \u001b[1;36m52\u001b[0m, seed \u001b[1;36m1874743937\u001b[0m\u001b[1m)\u001b[0m                   \n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "!git clone -b V20250909 https://github.com/Troyanovsky/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements/full/requirements.txt\n",
        "!pip install https://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.36.0/llama_cpp_binaries-0.36.0+cu124-py3-none-linux_x86_64.whl\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q4_K_M.gguf?download=true -d /content/text-generation-webui/user_data/models -o NousResearch_Hermes-4-14B-Q4_K_M.gguf\n",
        "%cd /content/text-generation-webui\n",
        "!python server.py --share --n-gpu-layers 999 --model NousResearch_Hermes-4-14B-Q4_K_M.gguf --n_ctx 8192"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODCzEdWE9fP9OwBgddE8cp",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}