{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Troyanovsky/Local-LLM-Comparison-Colab-UI/blob/main/Llama_3_2_3B_Instruct_Q8_0_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkpBtLvuTOp"
      },
      "source": [
        "## Llama-3.2-3B-Instruct-Q8_0-GGUF WebUI\n",
        "\n",
        "1. Run the following cell, takes ~5 min\n",
        "(You may need to confirm your GPU to proceed.)\n",
        "2. Pick the version you need from one of the last two cells and only run that cell.\n",
        "3. Click the gradio link at the bottom\n",
        "4. Instruction Template:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Cutting Knowledge Date: December 2023\n",
        "Today Date: 23 July 2024\n",
        "\n",
        "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "```\n",
        "\n",
        "Original model: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
        "\n",
        "Quantized model: https://huggingface.co/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF\n",
        "\n",
        "Want to try other local LLMs? Check out this repo: https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-MiHp_bveP6"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!apt-get -y install -qq aria2\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!git clone -b V20240929 https://github.com/Troyanovsky/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF/resolve/main/llama-3.2-3b-instruct-q8_0.gguf?download=true -d /content/text-generation-webui/models/ -o llama-3.2-3b-instruct-q8_0.gguf\n",
        "\n",
        "%cd /content/text-generation-webui\n",
        "!./start_linux.sh --share --n-gpu-layers 100000 --model llama-3.2-3b-instruct-q8_0.gguf --n_ctx 8196"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB49SwLl9u2Hko5sl5ogE1",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}