{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Troyanovsky/Local-LLM-Comparison-Colab-UI/blob/main/Llama_3_2_3B_Instruct_Q8_0_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkpBtLvuTOp"
      },
      "source": [
        "## Llama-3.2-3B-Instruct-Q8_0-GGUF WebUI\n",
        "\n",
        "1. Run the following cell, takes ~5 min\n",
        "(You may need to confirm your GPU to proceed.)\n",
        "2. Pick the version you need from one of the last two cells and only run that cell.\n",
        "3. Click the gradio link at the bottom\n",
        "4. Instruction Template:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Cutting Knowledge Date: December 2023\n",
        "Today Date: 23 July 2024\n",
        "\n",
        "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "```\n",
        "\n",
        "Original model: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
        "\n",
        "Quantized model: https://huggingface.co/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF\n",
        "\n",
        "Want to try other local LLMs? Check out this repo: https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-MiHp_bveP6"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!apt-get -y install -qq aria2\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!git clone -b V20240929 https://github.com/Troyanovsky/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF/resolve/main/llama-3.2-3b-instruct-q8_0.gguf?download=true -d /content/text-generation-webui/models/ -o llama-3.2-3b-instruct-q8_0.gguf\n",
        "\n",
        "%cd /content/text-generation-webui\n",
        "!./start_linux.sh --share --n-gpu-layers 100000 --model llama-3.2-3b-instruct-q8_0.gguf --n_ctx 8196"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/text-generation-webui\n",
        "!./start_linux.sh --share --n-gpu-layers 100000 --model llama-3.2-3b-instruct-q8_0.gguf --n_ctx 8196"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvuZL-bFOuOO",
        "outputId": "48b19425-05f3-4ee0-a5dc-fee25e13d61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/text-generation-webui\n",
            "\u001b[2;36m13:18:01-899956\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting Text generation web UI                                            \n",
            "\u001b[2;36m13:18:02-233601\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"llama-3.2-3b-instruct-q8_0.gguf\"\u001b[0m                                  \n",
            "\u001b[2;36m13:18:02-541794\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m llama.cpp weights detected: \u001b[32m\"models/llama-3.2-3b-instruct-q8_0.gguf\"\u001b[0m       \n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from models/llama-3.2-3b-instruct-q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  18:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q8_0:  197 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_head           = 24\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 3\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 3.21 B\n",
            "llm_load_print_meta: model size       = 3.18 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
            "llm_load_tensors: offloading 28 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 29/29 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   399.23 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  3255.91 MiB\n",
            ".................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 8224\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   899.50 MiB\n",
            "llama_new_context_with_model: KV self size  =  899.50 MiB, K (f16):  449.75 MiB, V (f16):  449.75 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   425.57 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    22.07 MiB\n",
            "llama_new_context_with_model: graph nodes  = 902\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'general.basename': 'Llama-3.2', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '24', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.2 3B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.block_count': '28', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
            "\n",
            "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "' }}\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n",
            "\u001b[2;36m13:18:04-567882\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded \u001b[32m\"llama-3.2-3b-instruct-q8_0.gguf\"\u001b[0m in \u001b[1;36m2.33\u001b[0m seconds.                  \n",
            "\u001b[2;36m13:18:04-569079\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOADER: \u001b[32m\"llama.cpp\"\u001b[0m                                                        \n",
            "\u001b[2;36m13:18:04-570067\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRUNCATION LENGTH: \u001b[1;36m8196\u001b[0m                                                    \n",
            "\u001b[2;36m13:18:04-570857\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INSTRUCTION TEMPLATE: \u001b[32m\"Custom \u001b[0m\u001b[32m(\u001b[0m\u001b[32mobtained from model metadata\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m              \n",
            "\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Running on public URL: https://c1e493a334d8dc1eab.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "/content/text-generation-webui/installer_files/env/lib/python3.11/site-packages/llama_cpp_cuda/llama.py:1138: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "\n",
            "llama_print_timings:        load time =     461.44 ms\n",
            "llama_print_timings:      sample time =    1117.46 ms /   146 runs   (    7.65 ms per token,   130.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =     460.42 ms /    28 tokens (   16.44 ms per token,    60.81 tokens per second)\n",
            "llama_print_timings:        eval time =    2550.75 ms /   145 runs   (   17.59 ms per token,    56.85 tokens per second)\n",
            "llama_print_timings:       total time =    5714.04 ms /   173 tokens\n",
            "Output generated in 6.30 seconds (23.00 tokens/s, 145 tokens, context 59, seed 2108609636)\n",
            "Llama.generate: 173 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =     461.44 ms\n",
            "llama_print_timings:      sample time =     221.21 ms /    29 runs   (    7.63 ms per token,   131.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =      49.85 ms /    28 tokens (    1.78 ms per token,   561.71 tokens per second)\n",
            "llama_print_timings:        eval time =     539.47 ms /    28 runs   (   19.27 ms per token,    51.90 tokens per second)\n",
            "llama_print_timings:       total time =    1114.16 ms /    56 tokens\n",
            "Output generated in 1.72 seconds (16.26 tokens/s, 28 tokens, context 262, seed 396312744)\n",
            "\u001b[2;36m13:20:11-696035\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Deleted \u001b[32m\"logs/instruct/20241001-13-19-10.json\"\u001b[0m.                            \n",
            "Llama.generate: 6 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =     461.44 ms\n",
            "llama_print_timings:      sample time =      62.71 ms /     8 runs   (    7.84 ms per token,   127.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =      43.40 ms /    23 tokens (    1.89 ms per token,   529.91 tokens per second)\n",
            "llama_print_timings:        eval time =     176.75 ms /     7 runs   (   25.25 ms per token,    39.60 tokens per second)\n",
            "llama_print_timings:       total time =     365.12 ms /    30 tokens\n",
            "Output generated in 0.99 seconds (7.04 tokens/s, 7 tokens, context 60, seed 1697163527)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn24bPti/cS7DLuUj199D3",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}